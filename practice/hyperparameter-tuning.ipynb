{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Support Vector Machines (SVM)\n",
    "\n",
    "Dataset: Iris flowers dataset\n",
    "\n",
    "Task: Optimize the hyperparameters of an SVM classifier (e.g., C and kernel type) to achieve the highest accuracy in classifying iris flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Random Forests\n",
    "\n",
    "Dataset: Wine quality dataset\n",
    "\n",
    "Task: Fine-tune the hyperparameters of a random forest classifier (e.g., number of trees, max depth) to maximize the F1 score in predicting the quality of wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Gradient Boosting\n",
    "\n",
    "Dataset: Credit card default dataset\n",
    "\n",
    "Task: Optimize the hyperparameters of a gradient boosting classifier (e.g., learning rate, maximum depth) to minimize the log loss in predicting credit card defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: K-Nearest Neighbors (KNN)\n",
    "\n",
    "Dataset: Customer churn dataset\n",
    "\n",
    "Task: Find the optimal number of neighbors (K) for a KNN classifier to maximize accuracy in predicting customer churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Multi-Layer Perceptron (MLP)\n",
    "Dataset: Handwritten digits dataset (MNIST)\n",
    "Task: Tune the hyperparameters of an MLP classifier (e.g., hidden layer sizes, learning rate) to achieve the highest accuracy in recognizing handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6: Ridge Regression\n",
    "\n",
    "Dataset: Housing prices dataset\n",
    "\n",
    "Task: Optimize the hyperparameter alpha in a ridge regression model to minimize the mean squared error in predicting house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7: Lasso Regression\n",
    "\n",
    "Dataset: Boston housing dataset\n",
    "\n",
    "Task: Fine-tune the hyperparameter alpha in a lasso regression model to achieve the lowest mean absolute error in predicting the median value of owner-occupied homes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8: AdaBoost Classifier\n",
    "\n",
    "Dataset: Bank marketing dataset\n",
    "\n",
    "Task: Find the optimal number of estimators for an AdaBoost classifier to maximize area under the ROC curve in predicting whether a client will subscribe to a term deposit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9: XGBoost Classifier\n",
    "\n",
    "Dataset: Titanic survival dataset\n",
    "\n",
    "Task: Tune the hyperparameters of an XGBoost classifier (e.g., learning rate, maximum depth) to maximize accuracy in predicting passenger survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10: Decision Tree Classifier\n",
    "\n",
    "Dataset: Breast cancer dataset\n",
    "\n",
    "Task: Optimize the hyperparameters of a decision tree classifier (e.g., max depth, minimum samples split) to achieve the highest accuracy in classifying breast tumors as benign or malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
